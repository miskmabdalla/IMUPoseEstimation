{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c969a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # Needed for plotting\n",
    "\n",
    "# def align_sequences(X, y):\n",
    "#     if X.shape[0] > y.shape[0]:\n",
    "#         X = X[:y.shape[0], :]\n",
    "#     elif y.shape[0] > X.shape[0]:\n",
    "#         delta = y.shape[0] - X.shape[0]\n",
    "#         cut_start = delta // 2\n",
    "#         cut_end = delta - cut_start\n",
    "#         y = y[cut_start: -cut_end if cut_end > 0 else None, :]\n",
    "#     return X, y\n",
    "\n",
    "# def load_trial_data(imu_trial_folder, angles_file, window_size):\n",
    "#     imu_dfs = []\n",
    "\n",
    "#     # Load each IMU CSV\n",
    "#     for fname in sorted(os.listdir(imu_trial_folder)):\n",
    "#         if fname.endswith(\".csv\"):\n",
    "#             path = os.path.join(imu_trial_folder, fname)\n",
    "#             df = pd.read_csv(path)\n",
    "\n",
    "#             print(f\"\\n--- Raw IMU File: {fname} ---\")\n",
    "#             print(df)\n",
    "\n",
    "#             # Drop timestamp (first column)\n",
    "#             df = df.drop(columns=[df.columns[0]])\n",
    "#             imu_dfs.append(df)\n",
    "\n",
    "#     if not imu_dfs:\n",
    "#         raise RuntimeError(f\"No IMU files found in {imu_trial_folder}\")\n",
    "\n",
    "#     # Determine the minimum shared length\n",
    "#     lengths = [df.shape[0] for df in imu_dfs]\n",
    "#     min_len = min(lengths)\n",
    "\n",
    "#     print(f\"[INFO] Truncating all IMU files to minimum length: {min_len} (original lengths: {lengths})\")\n",
    "\n",
    "#     # Truncate all IMU dataframes\n",
    "#     imu_dfs = [df.iloc[:min_len, :] for df in imu_dfs]\n",
    "#     imu_concat_df = pd.concat(imu_dfs, axis=1)\n",
    "\n",
    "#     print(f\"\\n=== Concatenated IMU Data (Trial: {os.path.basename(imu_trial_folder)}) ===\")\n",
    "#     print(imu_concat_df)\n",
    "#     print(f\"Shape: {imu_concat_df.shape}\")\n",
    "\n",
    "#     X = imu_concat_df.values\n",
    "\n",
    "#     # Load and clean angles\n",
    "#     y_df = pd.read_csv(angles_file)\n",
    "#     print(f\"\\n--- Angle File: {os.path.basename(angles_file)} ---\")\n",
    "#     print(y_df)\n",
    "\n",
    "#     y = y_df.drop(columns=[col for col in y_df.columns if 'time' in col.lower()]).values\n",
    "\n",
    "#     # Align sequence lengths between X and y\n",
    "#     X, y = align_sequences(X, y)\n",
    "\n",
    "#     # Optional: quick plot to verify y shape visually\n",
    "#     plt.plot(y[:500])\n",
    "#     plt.title(\"First 500 target values\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # Windowing\n",
    "#     X_seq, y_seq = [], []\n",
    "#     for i in range(len(X) - window_size + 1):\n",
    "#         X_seq.append(X[i:i + window_size])\n",
    "#         y_seq.append(y[i + window_size - 1])\n",
    "\n",
    "#     return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8709f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleSpec(name='torch', loader=<_frozen_importlib_external.SourceFileLoader object at 0x705f1830c1c0>, origin='/home/fadluw/anaconda3/envs/misk/lib/python3.10/site-packages/torch/__init__.py', submodule_search_locations=['/home/fadluw/anaconda3/envs/misk/lib/python3.10/site-packages/torch'])\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "torch_spec = importlib.util.find_spec(\"torch\")\n",
    "print(torch_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723a451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing trial_1\n",
      "[INFO] Processing trial_2\n",
      "[INFO] Processing trial_3\n",
      "[INFO] Processing trial_4\n",
      "[INFO] Finished processing. Total sequences: 35174\n",
      "Sample input shape: torch.Size([50, 12])\n",
      "Sample target shape: torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Ensure 'scripts' is in the module path\n",
    "sys.path.append('scripts')\n",
    "\n",
    "# Import your preprocessor\n",
    "from load_all_trials import load_and_preprocess_all_trials\n",
    "\n",
    "# Define data sources\n",
    "imu_data_root = os.path.abspath(\"dataset/imu_data\")\n",
    "angles_dir    = os.path.abspath(\"dataset/angles_data/csv_files\")\n",
    "window_size   = 50\n",
    "\n",
    "# Load, align, scale, window all data across trials\n",
    "X_all, y_all, scaler_X, scaler_y = load_and_preprocess_all_trials(\n",
    "    imu_data_root, angles_dir, window_size\n",
    ")\n",
    "\n",
    "# Train/val split\n",
    "# First split: train vs temp (val + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: val vs test (from temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Wrap in PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "x0, y0 = train_dataset[0]\n",
    "print(\"Sample input shape:\", x0.shape)\n",
    "print(\"Sample target shape:\", y0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9055804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IMULSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Use last time step\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0eb1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training started: 2025-04-07 05:40:05\n",
      "Device: CUDA\n",
      "Hyperparameters:\n",
      "  Hidden Size   = 128\n",
      "  Num Layers    = 2\n",
      "  Batch Size    = 32\n",
      "  Epochs        = 20\n",
      "  Learning Rate = 0.001\n",
      "  Window Size   = 50\n",
      "======================================================================\n",
      "\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "Epoch 1/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.9593\n",
      "  Batch  200/660 - Running MSE Loss: 0.8621\n",
      "  Batch  300/660 - Running MSE Loss: 0.8372\n",
      "  Batch  400/660 - Running MSE Loss: 0.7936\n",
      "  Batch  500/660 - Running MSE Loss: 0.7553\n",
      "  Batch  600/660 - Running MSE Loss: 0.6919\n",
      "  Batch  660/660 - Running MSE Loss: 0.4210\n",
      "  ↳ Train → MSE: 0.8064, MAE: 0.6120, RMSE: 0.8980, R²: 0.1786\n",
      "  ↳ Val   → MSE: 0.6997, MAE: 0.5688, RMSE: 0.8365, R²: 0.2702\n",
      "----------------------------------------------------------------------\n",
      "Epoch 2/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.6818\n",
      "  Batch  200/660 - Running MSE Loss: 0.6773\n",
      "  Batch  300/660 - Running MSE Loss: 0.6474\n",
      "  Batch  400/660 - Running MSE Loss: 0.6091\n",
      "  Batch  500/660 - Running MSE Loss: 0.6276\n",
      "  Batch  600/660 - Running MSE Loss: 0.6098\n",
      "  Batch  660/660 - Running MSE Loss: 0.3463\n",
      "  ↳ Train → MSE: 0.6363, MAE: 0.5181, RMSE: 0.7977, R²: 0.3518\n",
      "  ↳ Val   → MSE: 0.5594, MAE: 0.4806, RMSE: 0.7480, R²: 0.4158\n",
      "----------------------------------------------------------------------\n",
      "Epoch 3/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.5664\n",
      "  Batch  200/660 - Running MSE Loss: 0.5024\n",
      "  Batch  300/660 - Running MSE Loss: 0.5350\n",
      "  Batch  400/660 - Running MSE Loss: 0.5029\n",
      "  Batch  500/660 - Running MSE Loss: 0.4827\n",
      "  Batch  600/660 - Running MSE Loss: 0.4360\n",
      "  Batch  660/660 - Running MSE Loss: 0.2885\n",
      "  ↳ Train → MSE: 0.5019, MAE: 0.4350, RMSE: 0.7084, R²: 0.4887\n",
      "  ↳ Val   → MSE: 0.4446, MAE: 0.4077, RMSE: 0.6668, R²: 0.5350\n",
      "----------------------------------------------------------------------\n",
      "Epoch 4/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.4160\n",
      "  Batch  200/660 - Running MSE Loss: 0.4055\n",
      "  Batch  300/660 - Running MSE Loss: 0.3901\n",
      "  Batch  400/660 - Running MSE Loss: 0.3968\n",
      "  Batch  500/660 - Running MSE Loss: 0.3947\n",
      "  Batch  600/660 - Running MSE Loss: 0.4415\n",
      "  Batch  660/660 - Running MSE Loss: 0.2470\n",
      "  ↳ Train → MSE: 0.4078, MAE: 0.3817, RMSE: 0.6386, R²: 0.5835\n",
      "  ↳ Val   → MSE: 0.3493, MAE: 0.3527, RMSE: 0.5910, R²: 0.6360\n",
      "----------------------------------------------------------------------\n",
      "Epoch 5/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.3423\n",
      "  Batch  200/660 - Running MSE Loss: 0.3392\n",
      "  Batch  300/660 - Running MSE Loss: 0.3237\n",
      "  Batch  400/660 - Running MSE Loss: 0.3495\n",
      "  Batch  500/660 - Running MSE Loss: 0.3504\n",
      "  Batch  600/660 - Running MSE Loss: 0.3362\n",
      "  Batch  660/660 - Running MSE Loss: 0.2025\n",
      "  ↳ Train → MSE: 0.3401, MAE: 0.3416, RMSE: 0.5832, R²: 0.6524\n",
      "  ↳ Val   → MSE: 0.2966, MAE: 0.3152, RMSE: 0.5446, R²: 0.6892\n",
      "----------------------------------------------------------------------\n",
      "Epoch 6/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.2808\n",
      "  Batch  200/660 - Running MSE Loss: 0.2639\n",
      "  Batch  300/660 - Running MSE Loss: 0.2979\n",
      "  Batch  400/660 - Running MSE Loss: 0.2687\n",
      "  Batch  500/660 - Running MSE Loss: 0.3288\n",
      "  Batch  600/660 - Running MSE Loss: 0.3725\n",
      "  Batch  660/660 - Running MSE Loss: 0.2447\n",
      "  ↳ Train → MSE: 0.3116, MAE: 0.3214, RMSE: 0.5582, R²: 0.6812\n",
      "  ↳ Val   → MSE: 0.3993, MAE: 0.3779, RMSE: 0.6319, R²: 0.5833\n",
      "----------------------------------------------------------------------\n",
      "Epoch 7/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.3511\n",
      "  Batch  200/660 - Running MSE Loss: 0.2961\n",
      "  Batch  300/660 - Running MSE Loss: 0.2722\n",
      "  Batch  400/660 - Running MSE Loss: 0.2559\n",
      "  Batch  500/660 - Running MSE Loss: 0.2769\n",
      "  Batch  600/660 - Running MSE Loss: 0.2434\n",
      "  Batch  660/660 - Running MSE Loss: 0.1557\n",
      "  ↳ Train → MSE: 0.2806, MAE: 0.3047, RMSE: 0.5297, R²: 0.7131\n",
      "  ↳ Val   → MSE: 0.2246, MAE: 0.2679, RMSE: 0.4739, R²: 0.7646\n",
      "----------------------------------------------------------------------\n",
      "Epoch 8/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.2123\n",
      "  Batch  200/660 - Running MSE Loss: 0.2208\n",
      "  Batch  300/660 - Running MSE Loss: 0.2341\n",
      "  Batch  400/660 - Running MSE Loss: 0.2598\n",
      "  Batch  500/660 - Running MSE Loss: 0.2121\n",
      "  Batch  600/660 - Running MSE Loss: 0.1795\n",
      "  Batch  660/660 - Running MSE Loss: 0.1072\n",
      "  ↳ Train → MSE: 0.2161, MAE: 0.2624, RMSE: 0.4648, R²: 0.7788\n",
      "  ↳ Val   → MSE: 0.1736, MAE: 0.2336, RMSE: 0.4167, R²: 0.8184\n",
      "----------------------------------------------------------------------\n",
      "Epoch 9/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1723\n",
      "  Batch  200/660 - Running MSE Loss: 0.1752\n",
      "  Batch  300/660 - Running MSE Loss: 0.2029\n",
      "  Batch  400/660 - Running MSE Loss: 0.2023\n",
      "  Batch  500/660 - Running MSE Loss: 0.2300\n",
      "  Batch  600/660 - Running MSE Loss: 0.2035\n",
      "  Batch  660/660 - Running MSE Loss: 0.1435\n",
      "  ↳ Train → MSE: 0.2015, MAE: 0.2506, RMSE: 0.4489, R²: 0.7941\n",
      "  ↳ Val   → MSE: 0.2153, MAE: 0.2641, RMSE: 0.4640, R²: 0.7760\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1966\n",
      "  Batch  200/660 - Running MSE Loss: 0.1611\n",
      "  Batch  300/660 - Running MSE Loss: 0.1472\n",
      "  Batch  400/660 - Running MSE Loss: 0.1542\n",
      "  Batch  500/660 - Running MSE Loss: 0.1337\n",
      "  Batch  600/660 - Running MSE Loss: 0.1221\n",
      "  Batch  660/660 - Running MSE Loss: 0.0880\n",
      "  ↳ Train → MSE: 0.1519, MAE: 0.2192, RMSE: 0.3898, R²: 0.8442\n",
      "  ↳ Val   → MSE: 0.1337, MAE: 0.2053, RMSE: 0.3657, R²: 0.8598\n",
      "----------------------------------------------------------------------\n",
      "Epoch 11/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1254\n",
      "  Batch  200/660 - Running MSE Loss: 0.1264\n",
      "  Batch  300/660 - Running MSE Loss: 0.1311\n",
      "  Batch  400/660 - Running MSE Loss: 0.1864\n",
      "  Batch  500/660 - Running MSE Loss: 0.2465\n",
      "  Batch  600/660 - Running MSE Loss: 0.2065\n",
      "  Batch  660/660 - Running MSE Loss: 0.1057\n",
      "  ↳ Train → MSE: 0.1709, MAE: 0.2248, RMSE: 0.4135, R²: 0.8253\n",
      "  ↳ Val   → MSE: 0.1826, MAE: 0.2303, RMSE: 0.4273, R²: 0.8100\n",
      "----------------------------------------------------------------------\n",
      "Epoch 12/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1725\n",
      "  Batch  200/660 - Running MSE Loss: 0.1554\n",
      "  Batch  300/660 - Running MSE Loss: 0.1489\n",
      "  Batch  400/660 - Running MSE Loss: 0.1296\n",
      "  Batch  500/660 - Running MSE Loss: 0.1299\n",
      "  Batch  600/660 - Running MSE Loss: 0.1496\n",
      "  Batch  660/660 - Running MSE Loss: 0.1200\n",
      "  ↳ Train → MSE: 0.1524, MAE: 0.2121, RMSE: 0.3904, R²: 0.8451\n",
      "  ↳ Val   → MSE: 0.2169, MAE: 0.2564, RMSE: 0.4657, R²: 0.7758\n",
      "----------------------------------------------------------------------\n",
      "Epoch 13/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.2215\n",
      "  Batch  200/660 - Running MSE Loss: 0.1960\n",
      "  Batch  300/660 - Running MSE Loss: 0.1955\n",
      "  Batch  400/660 - Running MSE Loss: 0.1289\n",
      "  Batch  500/660 - Running MSE Loss: 0.1447\n",
      "  Batch  600/660 - Running MSE Loss: 0.1259\n",
      "  Batch  660/660 - Running MSE Loss: 0.0791\n",
      "  ↳ Train → MSE: 0.1654, MAE: 0.2225, RMSE: 0.4068, R²: 0.8306\n",
      "  ↳ Val   → MSE: 0.1311, MAE: 0.1976, RMSE: 0.3621, R²: 0.8632\n",
      "----------------------------------------------------------------------\n",
      "Epoch 14/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1220\n",
      "  Batch  200/660 - Running MSE Loss: 0.0933\n",
      "  Batch  300/660 - Running MSE Loss: 0.1195\n",
      "  Batch  400/660 - Running MSE Loss: 0.1076\n",
      "  Batch  500/660 - Running MSE Loss: 0.1006\n",
      "  Batch  600/660 - Running MSE Loss: 0.1098\n",
      "  Batch  660/660 - Running MSE Loss: 0.0593\n",
      "  ↳ Train → MSE: 0.1079, MAE: 0.1801, RMSE: 0.3285, R²: 0.8892\n",
      "  ↳ Val   → MSE: 0.1094, MAE: 0.1808, RMSE: 0.3308, R²: 0.8855\n",
      "----------------------------------------------------------------------\n",
      "Epoch 15/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.0906\n",
      "  Batch  200/660 - Running MSE Loss: 0.0789\n",
      "  Batch  300/660 - Running MSE Loss: 0.0898\n",
      "  Batch  400/660 - Running MSE Loss: 0.0826\n",
      "  Batch  500/660 - Running MSE Loss: 0.0837\n",
      "  Batch  600/660 - Running MSE Loss: 0.0708\n",
      "  Batch  660/660 - Running MSE Loss: 0.0376\n",
      "  ↳ Train → MSE: 0.0809, MAE: 0.1593, RMSE: 0.2845, R²: 0.9165\n",
      "  ↳ Val   → MSE: 0.0664, MAE: 0.1477, RMSE: 0.2577, R²: 0.9300\n",
      "----------------------------------------------------------------------\n",
      "Epoch 16/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.0603\n",
      "  Batch  200/660 - Running MSE Loss: 0.0726\n",
      "  Batch  300/660 - Running MSE Loss: 0.0836\n",
      "  Batch  400/660 - Running MSE Loss: 0.0647\n",
      "  Batch  500/660 - Running MSE Loss: 0.1842\n",
      "  Batch  600/660 - Running MSE Loss: 0.1902\n",
      "  Batch  660/660 - Running MSE Loss: 0.0914\n",
      "  ↳ Train → MSE: 0.1132, MAE: 0.1778, RMSE: 0.3365, R²: 0.8839\n",
      "  ↳ Val   → MSE: 0.1242, MAE: 0.1978, RMSE: 0.3524, R²: 0.8695\n",
      "----------------------------------------------------------------------\n",
      "Epoch 17/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.0960\n",
      "  Batch  200/660 - Running MSE Loss: 0.0943\n",
      "  Batch  300/660 - Running MSE Loss: 0.0915\n",
      "  Batch  400/660 - Running MSE Loss: 0.0740\n",
      "  Batch  500/660 - Running MSE Loss: 0.0659\n",
      "  Batch  600/660 - Running MSE Loss: 0.0630\n",
      "  Batch  660/660 - Running MSE Loss: 0.0416\n",
      "  ↳ Train → MSE: 0.0798, MAE: 0.1616, RMSE: 0.2824, R²: 0.9178\n",
      "  ↳ Val   → MSE: 0.0684, MAE: 0.1486, RMSE: 0.2615, R²: 0.9282\n",
      "----------------------------------------------------------------------\n",
      "Epoch 18/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.0704\n",
      "  Batch  200/660 - Running MSE Loss: 0.0585\n",
      "  Batch  300/660 - Running MSE Loss: 0.0596\n",
      "  Batch  400/660 - Running MSE Loss: 0.0603\n",
      "  Batch  500/660 - Running MSE Loss: 0.0807\n",
      "  Batch  600/660 - Running MSE Loss: 0.0653\n",
      "  Batch  660/660 - Running MSE Loss: 0.0588\n",
      "  ↳ Train → MSE: 0.0687, MAE: 0.1459, RMSE: 0.2621, R²: 0.9296\n",
      "  ↳ Val   → MSE: 0.0841, MAE: 0.1587, RMSE: 0.2899, R²: 0.9117\n",
      "----------------------------------------------------------------------\n",
      "Epoch 19/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.0771\n",
      "  Batch  200/660 - Running MSE Loss: 0.0662\n",
      "  Batch  300/660 - Running MSE Loss: 0.0589\n",
      "  Batch  400/660 - Running MSE Loss: 0.0547\n",
      "  Batch  500/660 - Running MSE Loss: 0.0507\n",
      "  Batch  600/660 - Running MSE Loss: 0.0720\n",
      "  Batch  660/660 - Running MSE Loss: 0.0957\n",
      "  ↳ Train → MSE: 0.0719, MAE: 0.1471, RMSE: 0.2682, R²: 0.9257\n",
      "  ↳ Val   → MSE: 0.1701, MAE: 0.2221, RMSE: 0.4124, R²: 0.8230\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20/20\n",
      "  Batch  100/660 - Running MSE Loss: 0.1520\n",
      "  Batch  200/660 - Running MSE Loss: 0.0930\n",
      "  Batch  300/660 - Running MSE Loss: 0.0746\n",
      "  Batch  400/660 - Running MSE Loss: 0.0626\n",
      "  Batch  500/660 - Running MSE Loss: 0.0574\n",
      "  Batch  600/660 - Running MSE Loss: 0.0546\n",
      "  Batch  660/660 - Running MSE Loss: 0.0335\n",
      "  ↳ Train → MSE: 0.0800, MAE: 0.1553, RMSE: 0.2828, R²: 0.9181\n",
      "  ↳ Val   → MSE: 0.0662, MAE: 0.1462, RMSE: 0.2573, R²: 0.9306\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Optional: R² helper function ---\n",
    "def r2_score(pred, target):\n",
    "    target_mean = torch.mean(target, dim=0)\n",
    "    ss_total = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - pred) ** 2)\n",
    "    return 1 - (ss_res / ss_total)\n",
    "\n",
    "\n",
    "# --- Logging Setup ---\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "log_dir = os.path.join('logs', f'run_{timestamp}')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "LOG_PATH = os.path.join(log_dir, 'training_log.txt')\n",
    "SAVE_STDOUT = True  # Toggle to also show logs in terminal\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_path = os.path.join(log_dir, 'metrics.csv')\n",
    "csv_fields = ['epoch', 'train_mse', 'train_mae', 'train_rmse', 'train_r2',\n",
    "              'val_mse', 'val_mae', 'val_rmse', 'val_r2']\n",
    "\n",
    "csv_file = open(csv_path, mode='w', newline='')\n",
    "csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n",
    "csv_writer.writeheader()\n",
    "\n",
    "\n",
    "from IPython.display import display  # Jupyter-safe\n",
    "from io import StringIO\n",
    "\n",
    "class DualLogger:\n",
    "    def __init__(self, filepath, print_to_stdout=True):\n",
    "        self.log_file = open(filepath, 'w')\n",
    "        self.print_to_stdout = print_to_stdout\n",
    "\n",
    "    def log(self, text):\n",
    "        self.log_file.write(text + '\\n')\n",
    "        self.log_file.flush()\n",
    "        if self.print_to_stdout:\n",
    "            print(text)\n",
    "\n",
    "    def close(self):\n",
    "        self.log_file.close()\n",
    "\n",
    "logger = DualLogger(LOG_PATH, print_to_stdout=True)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Header info\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"  Hidden Size   = {hidden_size}\")\n",
    "print(f\"  Num Layers    = {num_layers}\")\n",
    "print(f\"  Batch Size    = {batch_size}\")\n",
    "print(f\"  Epochs        = {epochs}\")\n",
    "print(f\"  Learning Rate = {learning_rate}\")\n",
    "print(f\"  Window Size   = {window_size}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Save hyperparameters to a JSON file alongside the log\n",
    "import json\n",
    "\n",
    "hyperparams = {\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"window_size\": window_size,\n",
    "}\n",
    "\n",
    "with open(os.path.join(log_dir, 'hyperparameters.json'), 'w') as f:\n",
    "    json.dump(hyperparams, f, indent=2)\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model and optimizer\n",
    "input_size = train_dataset[0][0].shape[1]\n",
    "output_size = train_dataset[0][1].shape[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = IMULSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "mae_loss_fn = nn.L1Loss()\n",
    "\n",
    "# Training loop\n",
    "logger.log(f\"\\nStarting training for {epochs} epochs...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    logger.log(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss_mse = 0\n",
    "    train_loss_mae = 0\n",
    "    train_r2_total = 0\n",
    "    train_samples = 0\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred = model(X_batch)\n",
    "        loss_mse = mse_loss_fn(pred, y_batch)\n",
    "        loss_mae = mae_loss_fn(pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_mse.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size_actual = X_batch.size(0)\n",
    "        train_loss_mse += loss_mse.item() * batch_size_actual\n",
    "        train_loss_mae += loss_mae.item() * batch_size_actual\n",
    "        train_r2_total += r2_score(pred, y_batch).item() * batch_size_actual\n",
    "        train_samples += batch_size_actual\n",
    "\n",
    "        running_loss += loss_mse.item()\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            avg_batch_loss = running_loss / min(100, (batch_idx + 1))\n",
    "            logger.log(f\"  Batch {batch_idx+1:>4}/{len(train_loader)} - Running MSE Loss: {avg_batch_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_train_mse = train_loss_mse / train_samples\n",
    "    avg_train_mae = train_loss_mae / train_samples\n",
    "    avg_train_rmse = avg_train_mse ** 0.5\n",
    "    avg_train_r2 = train_r2_total / train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_mse = 0\n",
    "    val_loss_mae = 0\n",
    "    val_r2_total = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            pred = model(X_val)\n",
    "\n",
    "            loss_mse = mse_loss_fn(pred, y_val)\n",
    "            loss_mae = mae_loss_fn(pred, y_val)\n",
    "\n",
    "            val_loss_mse += loss_mse.item() * X_val.size(0)\n",
    "            val_loss_mae += loss_mae.item() * X_val.size(0)\n",
    "            val_r2_total += r2_score(pred, y_val).item() * X_val.size(0)\n",
    "            val_samples += X_val.size(0)\n",
    "\n",
    "    avg_val_mse = val_loss_mse / val_samples\n",
    "    avg_val_mae = val_loss_mae / val_samples\n",
    "    avg_val_rmse = avg_val_mse ** 0.5\n",
    "    avg_val_r2 = val_r2_total / val_samples\n",
    "\n",
    "    # Final epoch summary\n",
    "    logger.log(f\"  ↳ Train → MSE: {avg_train_mse:.4f}, MAE: {avg_train_mae:.4f}, RMSE: {avg_train_rmse:.4f}, R²: {avg_train_r2:.4f}\")\n",
    "    logger.log(f\"  ↳ Val   → MSE: {avg_val_mse:.4f}, MAE: {avg_val_mae:.4f}, RMSE: {avg_val_rmse:.4f}, R²: {avg_val_r2:.4f}\")\n",
    "    logger.log(\"-\" * 70)\n",
    "\n",
    "\n",
    "    csv_writer.writerow({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_mse': avg_train_mse,\n",
    "        'train_mae': avg_train_mae,\n",
    "        'train_rmse': avg_train_rmse,\n",
    "        'train_r2': avg_train_r2,\n",
    "        'val_mse': avg_val_mse,\n",
    "        'val_mae': avg_val_mae,\n",
    "        'val_rmse': avg_val_rmse,\n",
    "        'val_r2': avg_val_r2,\n",
    "    })\n",
    "    csv_file.flush()  # Important to avoid loss on crashes\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e345d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train y mean/std: -0.00016199485980905592 1.006033182144165\n",
      "Val   y mean/std: 0.0032754098065197468 0.9947154521942139\n",
      "Train y min/max: -5.084049701690674 10.121524810791016\n",
      "Val   y min/max: -5.09799861907959 9.574485778808594\n"
     ]
    }
   ],
   "source": [
    "train_y = train_dataset[:][1]\n",
    "val_y = val_dataset[:][1]\n",
    "\n",
    "print(\"Train y mean/std:\", train_y.mean().item(), train_y.std().item())\n",
    "print(\"Val   y mean/std:\", val_y.mean().item(), val_y.std().item())\n",
    "print(\"Train y min/max:\", train_y.min().item(), train_y.max().item())\n",
    "print(\"Val   y min/max:\", val_y.min().item(), val_y.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3dcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"imu_lstm_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5057774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.0668\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define loss function if not already done\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Wrap test data (if not already done)\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Run evaluation\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        X_test_batch = X_test_batch.to(device)\n",
    "        y_test_batch = y_test_batch.to(device)\n",
    "\n",
    "        pred = model(X_test_batch)\n",
    "        loss = criterion(pred, y_test_batch)\n",
    "\n",
    "        test_loss += loss.item() * X_test_batch.size(0)\n",
    "\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_targets.append(y_test_batch.cpu())\n",
    "\n",
    "# Final loss value (mean over all samples)\n",
    "test_loss /= len(test_dataset)\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Optional: Convert to NumPy for metrics or plotting\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
